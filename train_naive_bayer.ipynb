{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.classify import util\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "from textblob.classifiers import DecisionTreeClassifier\n",
    "from nltk.stem.snowball import PortugueseStemmer\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normaliza_arquivos(data, sep):\n",
    "    dataset=[]\n",
    "    for line in data:\n",
    "        values = line.split(sep)\n",
    "        text = ','.join(values[0].split(',')[1:]).lower()\n",
    "        label = values[1].lower()\n",
    "        if len(text) > 1:\n",
    "            if label == 'bom':\n",
    "                label = 'b'\n",
    "            elif label == 'ruim':\n",
    "                label = 'r'\n",
    "            elif label == 'neutro':\n",
    "                label = 'n'\n",
    "            text = text.lower().replace('\"', '').replace(\"'\", \"\")\n",
    "            dataset.append((text, label))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lendo arquivo Fabiano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('treinamento/tweets_0_analise.csv', encoding='latin-1') as f:\n",
    "    data0 = f.readlines()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lendo arquivo Carlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('treinamento/tweets_1-PacoteDeMil.csv', encoding='utf-8') as f:\n",
    "    data1 = f.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('treinamento/tweets_2.csv', encoding='utf-8') as f:\n",
    "    data2 = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#LImpar arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "class WordProcessor():\n",
    "\n",
    "    url = re.compile('https*://.*( |.|,|!)+')\n",
    "    user_name = re.compile('@([0-9]|[A-z]|_| )+')\n",
    "    hashtag = re.compile('#([A-z]|[0-9]| )+')\n",
    "\n",
    "    def remove_url(self, text):\n",
    "        return ''.join(self.url.split(text))\n",
    "\n",
    "    def remove_user_mencions(self, text):\n",
    "        return ''.join(self.user_name.split(text))\n",
    "    \n",
    "    def remove_hash_tag(self, text):\n",
    "        return ''.join(self.hashtag.split(text))\n",
    "    \n",
    "    def remove_small_words(self, text, length):\n",
    "        return ' '.join([word for word in text.split(' ') if len(word)>length])\n",
    "    \n",
    "    def remove_pontuation(self, text):\n",
    "        return text.replace('!', '').replace('.', '').replace('?', '').replace(',', '')\n",
    "                    \n",
    "    \n",
    "    def remove_stop_words(self, text):\n",
    "        stop_words = stopwords.words('portuguese')\n",
    "        text = text.lower()\n",
    "        tokens = word_tokenize(text, language='portuguese')\n",
    "        return ' '.join([ word for word in tokens if not word  in stop_words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = normaliza_arquivos(data0, ';')\n",
    "dataset += normaliza_arquivos(data1, '\\t')\n",
    "dataset += normaliza_arquivos(data2, '\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpando arquivo\n",
    "\n",
    "No trecho abaixo sao removidas as mencoes a usuarios, as hashtags, url e as palavras de parada do portugues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('se eu fosse mulher do justin ia torcer pros nossos filhos puxarem ao pai pq que bicha estranha eu sou mano', 'n'), ('acabei de lavar o cabelo', 'n'), ('mai tá falando ainda?', 'n'), ('#trocadepasses , @rogerflorestv , acho que o novo técnico do flamengo pode ser o wagner mancini..!!', 'n'), ('prefiro ser sincero com as pessoas, mesmo que algumas não gostem, antes sincero e verdadeiro do que falso.', 'b'), ('@ifvccabello sdv bb', 'n'), ('depende https://t.co/72u8z7yh1w', 'n'), ('deus, eu te amo.', 'b'), ('hj parei p refletir como uma pessoa faz falta, como sua vida fica avulsa sem a presença de quem vc sempre tem por perto, então valoriza', 'b'), ('já avisei hj minha mãe que esse mês eu vou furar a orelha', 'n')]\n",
      "[['mulher justin ia torcer pros filhos puxarem pai pq bicha estranha man', 'n'], ['acabei lavar cabel', 'n'], ['mai tá falando ainda ', 'n'], ['acho novo técnico flamengo pode ser wagner mancini  ', 'n'], ['prefiro ser sincero pessoas  algumas gostem  antes sincero verdadeiro falso ', 'b'], ['depend', 'n'], ['deus  amo ', 'b'], ['hj parei refletir pessoa faz falta  vida fica avulsa presença vc sempre perto  então valoriz', 'b'], ['avisei hj mãe mês vou furar orelh', 'n'], ['ódio fico fazem questão mano  séri', 'r']]\n"
     ]
    }
   ],
   "source": [
    "wp = WordProcessor()\n",
    "\n",
    "stem = PortugueseStemmer()\n",
    "\n",
    "clean_data = []\n",
    "for sentence, classification in dataset:\n",
    "    text_clean = wp.remove_url(sentence)\n",
    "    text_clean = wp.remove_user_mencions(text_clean)\n",
    "    text_clean = wp.remove_hash_tag(text_clean)\n",
    "    text_clean = wp.remove_small_words(text_clean, 1)\n",
    "    text_clean = wp.remove_stop_words(text_clean)\n",
    "    text_clean = wp.remove_pontuation(text_clean)\n",
    "    text_clean = stem.stem(text_clean)\n",
    "    if len(text_clean) > 3:\n",
    "        \n",
    "        clean_data.append([text_clean, classification])\n",
    "\n",
    "print(dataset[:10])\n",
    "print(clean_data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando teste\n",
    "\n",
    "Separando massa em train e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "train_index = np.random.randint(len(clean_data), size=int(len(clean_data)*0.8), dtype=int)\n",
    "train = [clean_data[i] for i in train_index]\n",
    "test_index = np.random.randint(len(clean_data), size=int(len(clean_data)*0.2),dtype=int)\n",
    "test = [clean_data[i] for i in test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = NaiveBayesClassifier(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7421259842519685\n"
     ]
    }
   ],
   "source": [
    "print(cl.accuracy(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "           contains(nao) = True                r : b      =     13.8 : 1.0\n",
      "           contains(amo) = True                b : n      =     11.3 : 1.0\n",
      "        contains(melhor) = True                b : r      =     10.2 : 1.0\n",
      "           contains(the) = True                n : r      =      9.4 : 1.0\n",
      "          contains(cara) = True                r : b      =      8.2 : 1.0\n",
      "           contains(tão) = True                r : n      =      7.6 : 1.0\n",
      "         contains(porra) = True                r : b      =      7.4 : 1.0\n",
      "           contains(dia) = True                b : r      =      7.4 : 1.0\n",
      "         contains(tomar) = True                r : n      =      7.3 : 1.0\n",
      "           contains(vem) = True                n : b      =      6.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "cl.show_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5885826771653543\n"
     ]
    }
   ],
   "source": [
    "print(dt.accuracy(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Devido a acuracia e o tempo de treinamento sera utilizado o algortimo de naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#serializando o treinamento\n",
    "with open('class_nb.bin', 'wb') as f:\n",
    "    pickle.dump(cl, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
