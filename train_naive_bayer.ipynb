{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normaliza_arquivos(data, sep):\n",
    "    dataset=[]\n",
    "    for line in data:\n",
    "        values = line.split(sep)\n",
    "        text = ','.join(values[0].split(',')[1:]).lower()\n",
    "        label = values[1].lower()\n",
    "        if len(text) > 1:\n",
    "            if label == 'bom':\n",
    "                label = 'b'\n",
    "            elif label == 'ruim':\n",
    "                label = 'r'\n",
    "            elif label == 'neutro':\n",
    "                label = 'n'\n",
    "            text = text.lower().replace('\"', '').replace(\"'\", \"\")\n",
    "            dataset.append((text, label))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lendo arquivo Fabiano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('treinamento/tweets_0_analise.csv', encoding='latin-1') as f:\n",
    "    data0 = f.readlines()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lendo arquivo Carlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('treinamento/tweets_1-PacoteDeMil.csv', encoding='utf-8') as f:\n",
    "    data1 = f.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('treinamento/tweets_2.csv', encoding='utf-8') as f:\n",
    "    data2 = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#LImpar arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "class WordProcessor():\n",
    "\n",
    "    url = re.compile('https*://.*( |.|,|!)+')\n",
    "    user_name = re.compile('@([0-9]|[A-z]|_| )+')\n",
    "    hashtag = re.compile('#([A-z]|[0-9]| )+')\n",
    "\n",
    "    def remove_url(self, text):\n",
    "        return ''.join(self.url.split(text))\n",
    "\n",
    "    def remove_user_mencions(self, text):\n",
    "        return ''.join(self.user_name.split(text))\n",
    "    \n",
    "    def remove_hash_tag(self, text):\n",
    "        return ''.join(self.hashtag.split(text))\n",
    "    \n",
    "    def remove_small_words(self, text, length):\n",
    "        return ' '.join([word for word in text.split(' ') if len(word)>length])\n",
    "    \n",
    "    def remove_pontuation(self, text):\n",
    "        return text.replace('!', '').replace('.', '').replace('?', '').replace(',', '')\n",
    "                    \n",
    "    \n",
    "    def remove_stop_words(self, text):\n",
    "        stop_words = stopwords.words('portuguese')\n",
    "        text = text.lower()\n",
    "        tokens = word_tokenize(text, language='portuguese')\n",
    "        return ' '.join([ word for word in tokens if not word  in stop_words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = normaliza_arquivos(data0, ';')\n",
    "dataset += normaliza_arquivos(data1, '\\t')\n",
    "dataset += normaliza_arquivos(data2, '\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpando arquivo\n",
    "\n",
    "No trecho abaixo sao removidas as mencoes a usuarios, as hashtags, url e as palavras de parada do portugues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('se eu fosse mulher do justin ia torcer pros nossos filhos puxarem ao pai pq que bicha estranha eu sou mano', 'n'), ('acabei de lavar o cabelo', 'n'), ('mai tá falando ainda?', 'n'), ('#trocadepasses , @rogerflorestv , acho que o novo técnico do flamengo pode ser o wagner mancini..!!', 'n'), ('prefiro ser sincero com as pessoas, mesmo que algumas não gostem, antes sincero e verdadeiro do que falso.', 'b'), ('@ifvccabello sdv bb', 'n'), ('depende https://t.co/72u8z7yh1w', 'n'), ('deus, eu te amo.', 'b'), ('hj parei p refletir como uma pessoa faz falta, como sua vida fica avulsa sem a presença de quem vc sempre tem por perto, então valoriza', 'b'), ('já avisei hj minha mãe que esse mês eu vou furar a orelha', 'n')]\n",
      "[['mulher justin ia torcer pros filhos puxarem pai pq bicha estranha mano', 'n'], ['acabei lavar cabelo', 'n'], ['mai tá falando ainda ', 'n'], ['acho novo técnico flamengo pode ser wagner mancini  ', 'n'], ['prefiro ser sincero pessoas  algumas gostem  antes sincero verdadeiro falso ', 'b'], ['depende', 'n'], ['deus  amo ', 'b'], ['hj parei refletir pessoa faz falta  vida fica avulsa presença vc sempre perto  então valoriza', 'b'], ['avisei hj mãe mês vou furar orelha', 'n'], ['ódio fico fazem questão mano  sério', 'r']]\n"
     ]
    }
   ],
   "source": [
    "wp = WordProcessor()\n",
    "\n",
    "clean_data = []\n",
    "for sentence, classification in dataset:\n",
    "    text_clean = wp.remove_url(sentence)\n",
    "    text_clean = wp.remove_user_mencions(text_clean)\n",
    "    text_clean = wp.remove_hash_tag(text_clean)\n",
    "    text_clean = wp.remove_small_words(text_clean, 1)\n",
    "    text_clean = wp.remove_stop_words(text_clean)\n",
    "    text_clean = wp.remove_pontuation(text_clean)\n",
    "    if len(text_clean) > 3:\n",
    "        clean_data.append([text_clean, classification])\n",
    "\n",
    "print(dataset[:10])\n",
    "print(clean_data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando teste\n",
    "\n",
    "Separando massa em train e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "train_index = np.random.randint(len(clean_data), size=int(len(clean_data)*0.8), dtype=int)\n",
    "train = [clean_data[i] for i in train_index]\n",
    "test_index = np.random.randint(len(clean_data), size=int(len(clean_data)*0.2),dtype=int)\n",
    "test = [clean_data[i] for i in test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = NaiveBayesClassifier(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textblob.classifiers.NaiveBayesClassifier"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = 0\n",
    "for i in test:\n",
    "    text = i[0]\n",
    "    res = cl.classify(text)\n",
    "    if res == i[1]:\n",
    "        result+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q=cl.prob_classify(\"isso eh triste\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'r'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r\n"
     ]
    }
   ],
   "source": [
    "print(q.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#serializando o treinamento\n",
    "cl = NaiveBayesClassifier(clean_data)\n",
    "with open('class_nb.bin', 'wb') as f:\n",
    "    pickle.dump(cl, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
